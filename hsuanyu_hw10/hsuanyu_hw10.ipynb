{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats 701-Homework10 Winter 2018\n",
    "## Hsuan-Yu Yeh \n",
    "### hsuanyu@umich.edu\n",
    "I discussed with In-Son Tseng and David Li for this homework. For problem 1, I spent 2 hours to finish it. For problem 2, I spent about 8 hours. For problem 3, I spent 2 hours for it. For problem 4, I spent 6 hours for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Constructing a 3-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# create numpy array first\n",
    "ary_tall_0 = np.array([ [0, 0, 1], \n",
    "                         [0, 0, 1],\n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1] ])\n",
    "ary_tall_1 = np.array([ [0, 0, 0], \n",
    "                         [0, 0, 0],\n",
    "                         [0, 0, 1],\n",
    "                         [0, 0, 0] ])\n",
    "ary_tall_2 = np.array([ [0, 0, 0], \n",
    "                         [0, 0, 0],\n",
    "                         [0, 1, 1],\n",
    "                         [0, 0, 0] ])\n",
    "ary_tall_3 = np.array([ [0, 0, 0], \n",
    "                         [0, 0, 0],\n",
    "                         [0, 0, 1],\n",
    "                         [0, 0, 0] ])\n",
    "ary_tall_4 = np.array([ [0, 0, 0], \n",
    "                         [0, 0, 0],\n",
    "                         [0, 0, 1],\n",
    "                         [0, 0, 0] ])\n",
    "ary_all = np.array([ary_tall_0, ary_tall_1, ary_tall_2, ary_tall_3, ary_tall_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the tensor is:(5, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# check the shape\n",
    "print(\"The shape of the tensor is:{}\".format(ary_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 3],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 2, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the T\n",
    "np.sum(ary_all, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 4],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the F\n",
    "np.sum(ary_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tf constant\n",
    "tflogo = tf.constant(ary_all)\n",
    "T_part = tf.reduce_sum(tflogo, axis=2)\n",
    "F_part = tf.reduce_sum(tflogo, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T:\n",
      "[[1 1 1 3]\n",
      " [0 0 1 0]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]]\n",
      "The F:\n",
      "[[1 1 4]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# check the \"T\" and \"F\" \n",
    "with tf.Session() as sess:\n",
    "    print(\"The T:\")\n",
    "    print(sess.run(T_part))\n",
    "    print(\"The F:\")\n",
    "    print(sess.run(F_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Building and training simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2-1\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.chdir('/Users/yehhsuan-yu/Umich/Stats701/hsuanyu_hw10')\n",
    "\n",
    "W = tf.Variable(np.random.randn(6, 1).astype(np.float32))\n",
    "b = tf.Variable(np.random.randn(1).astype(np.float32))\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 6])\n",
    "beta = tf.matmul(x, W) + b\n",
    "\n",
    "y = tf.placeholder(tf.int64, [None, 1])\n",
    "\n",
    "hyp = tf.sigmoid(beta)\n",
    "\n",
    "y_float = tf.cast(y, np.float32)\n",
    "\n",
    "cost0 = y_float * tf.log(hyp)\n",
    "cost1 = (1 - y_float) * tf.log(1 - hyp)\n",
    "cost = (cost0 + cost1)\n",
    "loss = tf.reduce_sum(-cost)\n",
    "\n",
    "lr = tf.Variable(1e-3, trainable = False)\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "W = [-8.4392422e-01 -8.3498083e-02  8.5094199e-04  1.2234355e+00\n",
      "  3.0739734e-01  6.8930119e-01]\n",
      "b = [-0.15905225]\n",
      "Accuracy:\n",
      "   train: 0.605500\n",
      "   test: 0.680000\n",
      "Loss:\n",
      "   train: 1318.547607\n",
      "   test: 304.902893\n",
      "Step: 100\n",
      "W = [0.7172959  0.58785075 1.506111   1.9990709  3.2201698  5.255762  ]\n",
      "b = [-0.6610426]\n",
      "Accuracy:\n",
      "   train: 0.846500\n",
      "   test: 0.866000\n",
      "Loss:\n",
      "   train: 704.762756\n",
      "   test: 170.856323\n",
      "Step: 200\n",
      "W = [0.8802521 0.6997496 1.8324376 2.4076731 3.9188828 6.426636 ]\n",
      "b = [-0.74762684]\n",
      "Accuracy:\n",
      "   train: 0.847500\n",
      "   test: 0.862000\n",
      "Loss:\n",
      "   train: 681.255859\n",
      "   test: 164.300735\n",
      "Step: 300\n",
      "W = [0.94165546 0.7451122  1.9801438  2.6111917  4.241353   6.9699187 ]\n",
      "b = [-0.7899094]\n",
      "Accuracy:\n",
      "   train: 0.847500\n",
      "   test: 0.860000\n",
      "Loss:\n",
      "   train: 676.327148\n",
      "   test: 162.758133\n",
      "Step: 400\n",
      "W = [0.97287667 0.7682496  2.0578425  2.7203648  4.4119835  7.2575235 ]\n",
      "b = [-0.8127134]\n",
      "Accuracy:\n",
      "   train: 0.848000\n",
      "   test: 0.858000\n",
      "Loss:\n",
      "   train: 674.957458\n",
      "   test: 162.254181\n",
      "Step: 500\n",
      "W = [0.9902363  0.78106314 2.1012392  2.7816432  4.507501   7.418494  ]\n",
      "b = [-0.8255883]\n",
      "Accuracy:\n",
      "   train: 0.848000\n",
      "   test: 0.858000\n",
      "Loss:\n",
      "   train: 674.529968\n",
      "   test: 162.057922\n",
      "Step: 600\n",
      "W = [1.0002253 0.7884132 2.1261957 2.8169377 4.56249   7.511145 ]\n",
      "b = [-0.8330329]\n",
      "Accuracy:\n",
      "   train: 0.847500\n",
      "   test: 0.858000\n",
      "Loss:\n",
      "   train: 674.388428\n",
      "   test: 161.971451\n",
      "Step: 700\n",
      "W = [1.0060667  0.79270333 2.1407747  2.8375673  4.594629   7.565291  ]\n",
      "b = [-0.83739465]\n",
      "Accuracy:\n",
      "   train: 0.847500\n",
      "   test: 0.856000\n",
      "Loss:\n",
      "   train: 674.340210\n",
      "   test: 161.929642\n",
      "Step: 800\n",
      "W = [1.0095117 0.7952305 2.1493676 2.8497274 4.613576  7.5972095]\n",
      "b = [-0.8399696]\n",
      "Accuracy:\n",
      "   train: 0.847500\n",
      "   test: 0.854000\n",
      "Loss:\n",
      "   train: 674.323486\n",
      "   test: 161.907944\n",
      "Step: 900\n",
      "W = [1.0115533  0.79672736 2.154457   2.8569312  4.6247993  7.616115  ]\n",
      "b = [-0.8414961]\n",
      "Accuracy:\n",
      "   train: 0.847500\n",
      "   test: 0.854000\n",
      "Loss:\n",
      "   train: 674.317627\n",
      "   test: 161.896103\n"
     ]
    }
   ],
   "source": [
    "# Q2-2,3\n",
    "\n",
    "data_dir = 'logistic_data'\n",
    "xtrain = np.load(os.path.join(data_dir,'logistic_xtrain.npy'))\n",
    "xtest = np.load(os.path.join(data_dir,'logistic_xtest.npy'))\n",
    "ytrain = np.load(os.path.join(data_dir,'logistic_ytrain.npy'))\n",
    "ytest = np.load(os.path.join(data_dir,'logistic_ytest.npy'))\n",
    "\n",
    "prediction = tf.cast(tf.round(hyp), tf.int64)\n",
    "correct_prediction = tf.equal(prediction, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "num_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for step in range(num_steps):       \n",
    "        _, W_, b_, l_, accu_train = sess.run([train_step, W, b, loss, accuracy], feed_dict={x: xtrain, y: ytrain})\n",
    "        l_test, accu_test = sess.run([loss,accuracy], feed_dict={x: xtest, y: ytest})\n",
    "        if step % 100 == 0:\n",
    "            print('Step: {}'.format(step))\n",
    "            print('W = {}'.format(W_.flatten()))\n",
    "            print('b = {}'.format(b_.flatten()))\n",
    "            print('Accuracy:')\n",
    "            print('   train: {:.6f}'.format(accu_train))\n",
    "            print('   test: {:.6f}'.format(accu_test))\n",
    "            print('Loss:')\n",
    "            print('   train: {:.6f}'.format(l_))\n",
    "            print('   test: {:.6f}'.format(l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3325267e-04 4.1322723e-02 2.3849113e-02 2.0478368e-02 1.4081675e-01\n",
      "  1.4743610e-01 2.5125807e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Q2-4\n",
    "W0 = tf.placeholder(tf.float32, [None, 7])\n",
    "W = tf.placeholder(tf.float32, [None, 7])\n",
    "se = tf.pow(W0-W, 2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(se, feed_dict={W: [[1.0115435, 0.7967201, 2.1544316, 2.8568974, 4.6247444, 7.616026, -0.8414888]], W0: [[1,1,2,3,5,8,-1]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "mu = [-0.067883    0.00103171  0.09433642]\n",
      "var = [1.0832015 1.0010877 1.2086453]\n",
      "Cross entropy:\n",
      "  train: 2.908446\n",
      "  test: 2.459063\n",
      "Accuracy:\n",
      "  train: 0.298000\n",
      "  test: 0.698000\n",
      "step: 100\n",
      "mu = [-0.97428054  0.00807605  1.2575095 ]\n",
      "var = [0.51148903 1.0041908  3.1596632 ]\n",
      "Cross entropy:\n",
      "  train: 1.555973\n",
      "  test: 1.516076\n",
      "Accuracy:\n",
      "  train: 0.720500\n",
      "  test: 0.716000\n",
      "step: 200\n",
      "mu = [-0.9809782   0.00807606  1.6943218 ]\n",
      "var = [0.5114177 1.0041908 3.3215592]\n",
      "Cross entropy:\n",
      "  train: 1.492037\n",
      "  test: 1.457844\n",
      "Accuracy:\n",
      "  train: 0.721500\n",
      "  test: 0.718000\n",
      "step: 300\n",
      "mu = [-0.98101026  0.00807606  2.0057266 ]\n",
      "var = [0.5114177 1.0041908 3.1582181]\n",
      "Cross entropy:\n",
      "  train: 1.457420\n",
      "  test: 1.425922\n",
      "Accuracy:\n",
      "  train: 0.723000\n",
      "  test: 0.716000\n",
      "step: 400\n",
      "mu = [-0.98101026  0.00807606  2.2518444 ]\n",
      "var = [0.5114177 1.0041908 2.8635774]\n",
      "Cross entropy:\n",
      "  train: 1.431830\n",
      "  test: 1.402335\n",
      "Accuracy:\n",
      "  train: 0.725000\n",
      "  test: 0.714000\n",
      "step: 500\n",
      "mu = [-0.98101026  0.00807606  2.4528224 ]\n",
      "var = [0.5114177 1.0041908 2.5258417]\n",
      "Cross entropy:\n",
      "  train: 1.411219\n",
      "  test: 1.383544\n",
      "Accuracy:\n",
      "  train: 0.725000\n",
      "  test: 0.712000\n",
      "step: 600\n",
      "mu = [-0.98101026  0.00807606  2.6164482 ]\n",
      "var = [0.5114177 1.0041908 2.2001884]\n",
      "Cross entropy:\n",
      "  train: 1.395019\n",
      "  test: 1.369170\n",
      "Accuracy:\n",
      "  train: 0.726000\n",
      "  test: 0.712000\n",
      "step: 700\n",
      "mu = [-0.98101026  0.00807606  2.7455566 ]\n",
      "var = [0.5114177 1.0041908 1.9239994]\n",
      "Cross entropy:\n",
      "  train: 1.383547\n",
      "  test: 1.359542\n",
      "Accuracy:\n",
      "  train: 0.728000\n",
      "  test: 0.714000\n",
      "step: 800\n",
      "mu = [-0.98101026  0.00807606  2.841924  ]\n",
      "var = [0.5114177 1.0041908 1.7172401]\n",
      "Cross entropy:\n",
      "  train: 1.376681\n",
      "  test: 1.354409\n",
      "Accuracy:\n",
      "  train: 0.727000\n",
      "  test: 0.716000\n",
      "step: 900\n",
      "mu = [-0.98101026  0.00807606  2.9089983 ]\n",
      "var = [0.5114177 1.0041908 1.5805749]\n",
      "Cross entropy:\n",
      "  train: 1.373326\n",
      "  test: 1.352487\n",
      "Accuracy:\n",
      "  train: 0.728000\n",
      "  test: 0.716000\n",
      "step: 1000\n",
      "mu = [-0.98101026  0.00807606  2.952605  ]\n",
      "var = [0.5114177 1.0041908 1.4997761]\n",
      "Cross entropy:\n",
      "  train: 1.371975\n",
      "  test: 1.352169\n",
      "Accuracy:\n",
      "  train: 0.728500\n",
      "  test: 0.720000\n",
      "step: 1100\n",
      "mu = [-0.98101026  0.00807606  2.979481  ]\n",
      "var = [0.5114177 1.0041908 1.4559373]\n",
      "Cross entropy:\n",
      "  train: 1.371503\n",
      "  test: 1.352363\n",
      "Accuracy:\n",
      "  train: 0.728500\n",
      "  test: 0.720000\n",
      "step: 1200\n",
      "mu = [-0.98101026  0.00807606  2.99548   ]\n",
      "var = [0.5114177 1.0041908 1.4334804]\n",
      "Cross entropy:\n",
      "  train: 1.371352\n",
      "  test: 1.352610\n",
      "Accuracy:\n",
      "  train: 0.729500\n",
      "  test: 0.720000\n",
      "step: 1300\n",
      "mu = [-0.98101026  0.00807606  3.0048158 ]\n",
      "var = [0.5114177 1.0041908 1.4223747]\n",
      "Cross entropy:\n",
      "  train: 1.371305\n",
      "  test: 1.352792\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n",
      "step: 1400\n",
      "mu = [-0.98101026  0.00807606  3.0102088 ]\n",
      "var = [0.5114177 1.0041908 1.4169931]\n",
      "Cross entropy:\n",
      "  train: 1.371291\n",
      "  test: 1.352907\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n",
      "step: 1500\n",
      "mu = [-0.98101026  0.00807606  3.0133061 ]\n",
      "var = [0.5114177 1.0041908 1.4144174]\n",
      "Cross entropy:\n",
      "  train: 1.371286\n",
      "  test: 1.352975\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n",
      "step: 1600\n",
      "mu = [-0.98101026  0.00807606  3.0150814 ]\n",
      "var = [0.5114177 1.0041908 1.4131926]\n",
      "Cross entropy:\n",
      "  train: 1.371285\n",
      "  test: 1.353015\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n",
      "step: 1700\n",
      "mu = [-0.98101026  0.00807606  3.016097  ]\n",
      "var = [0.5114177 1.0041908 1.4126126]\n",
      "Cross entropy:\n",
      "  train: 1.371285\n",
      "  test: 1.353037\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n",
      "step: 1800\n",
      "mu = [-0.98101026  0.00807606  3.0166788 ]\n",
      "var = [0.5114177 1.0041908 1.412338 ]\n",
      "Cross entropy:\n",
      "  train: 1.371285\n",
      "  test: 1.353050\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n",
      "step: 1900\n",
      "mu = [-0.98101026  0.00807606  3.017011  ]\n",
      "var = [0.5114177 1.0041908 1.4122099]\n",
      "Cross entropy:\n",
      "  train: 1.371284\n",
      "  test: 1.353057\n",
      "Accuracy:\n",
      "  train: 0.729000\n",
      "  test: 0.720000\n"
     ]
    }
   ],
   "source": [
    "# Q2-5,6,8\n",
    "data_dir = 'normal_data'\n",
    "xtrain_nm = np.load(os.path.join(data_dir,'normal_xtrain.npy'))\n",
    "xtest_nm = np.load(os.path.join(data_dir,'normal_xtest.npy'))\n",
    "ytrain_nm = np.load(os.path.join(data_dir,'normal_ytrain.npy'))\n",
    "ytest_nm = np.load(os.path.join(data_dir,'normal_ytest.npy'))\n",
    "\n",
    "\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "y = tf.placeholder(dtype = tf.float32, shape=[None, 3])\n",
    "\n",
    "mu = tf.Variable(np.zeros(3).astype(np.float32))\n",
    "sigma = tf.Variable(np.ones(3).astype(np.float32))\n",
    "\n",
    "xent = 0\n",
    "negative_log_p = []\n",
    "for k in range(3):\n",
    "    negative_log_p.append(tf.log(2 * np.pi * sigma[k]**2) + (x - mu[k])**2 / (sigma[k]**2))\n",
    "    xent = xent + tf.reduce_mean(y[:, k] * tf.squeeze(negative_log_p[k])) / 2\n",
    "\n",
    "negative_log_p = tf.concat(negative_log_p, axis=1)\n",
    "pred = tf.argmin(negative_log_p, axis = 1)\n",
    "label = tf.argmax(y, axis = 1)\n",
    "accu = tf.reduce_mean(tf.cast(tf.equal(pred, label), tf.float32))\n",
    "\n",
    "lr = tf.Variable(1e-1, trainable = False)\n",
    "train_op = tf.train.AdagradOptimizer(lr).minimize(xent)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "num_steps = 2000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for step in range(num_steps):\n",
    "        _, xent_train, accu_train = sess.run([train_op, xent, accu], feed_dict = {x: xtrain_nm, y: ytrain_nm})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            xent_test, accu_test = sess.run([xent, accu], feed_dict = {x: xtest_nm, y: ytest_nm})\n",
    "            mu_, sigma_ = sess.run([mu, sigma])\n",
    "            \n",
    "            print('step: {}'.format(step))\n",
    "            print('mu = {}'.format(mu_))\n",
    "            print('var = {}'.format(sigma_**2))\n",
    "            \n",
    "            print('Cross entropy:')\n",
    "            print('  train: {:.6f}'.format(xent_train))\n",
    "            print('  test: {:.6f}'.format(xent_test))\n",
    "            \n",
    "            print('Accuracy:')\n",
    "            print('  train: {:.6f}'.format(accu_train))\n",
    "            print('  test: {:.6f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008536565\n"
     ]
    }
   ],
   "source": [
    "# 2-7\n",
    "param0 = tf.placeholder(tf.float32, [None, 6])\n",
    "param = tf.placeholder(tf.float32, [None, 6])\n",
    "sse = tf.reduce_sum(tf.pow(param-param0, 2))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(sse, feed_dict={param: [[-0.98101026, 0.00807606, 3.0166788, 0.5114177, 1.0041908, 1.412338]], param0: [[-1, 0, 3, 0.5, 1, 1.5]]}))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# save the model for problem 4\n",
    "mu0 = tf.constant([-1.0], dtype = tf.float32)\n",
    "mu1 = tf.constant([0.0], dtype = tf.float32)\n",
    "mu2 = tf.constant([3.0], dtype = tf.float32)\n",
    "sigma0 = tf.constant([0.5], dtype = tf.float32)\n",
    "sigma1 = tf.constant([1.0], dtype = tf.float32)\n",
    "sigma2 = tf.constant([1.5], dtype = tf.float32)\n",
    "distribnorm = tf.distributions.Normal(loc = [mu0, mu1, mu2], scale = [tf.sqrt(sigma0), tf.sqrt(sigma1), tf.sqrt(sigma2)])\n",
    "p = distribnorm.prob([x, x, x])\n",
    "pred = tf.argmax(p)[0]\n",
    "with tf.Session() as sess:\n",
    "   print(sess.run(pred, feed_dict = {x: 4.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'/Users/yehhsuan-yu/Umich/Stats701/hsuanyu_hw10/hsuanyu_normal_trained/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    tf.saved_model.simple_save(session = sess,\n",
    "                              export_dir = '/Users/yehhsuan-yu/Umich/Stats701/hsuanyu_hw10/hsuanyu_normal_trained',\n",
    "                              inputs = {'x': x},\n",
    "                              outputs = {'prediction': pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Train the CNN MNIST classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-69-8df3d25ba548>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:219: retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# 3 \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-71-42352af42809>:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input variables\n",
    "x_input = tf.placeholder(tf.float32, [None, 784])\n",
    "y_input = tf.placeholder(tf.float32, [None, 10])\n",
    "x_img_input = tf.reshape(x_input, [-1, 28, 28, 1])\n",
    "\n",
    "# dropout setting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# parameters for conv_1\n",
    "W_conv_1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv_1 = bias_variable([32])\n",
    "\n",
    "conv_1 = conv2d(x_img_input, W_conv_1) + b_conv_1\n",
    "ReLU_1 = tf.nn.relu(conv_1)\n",
    "pool_1 = max_pool_2x2(ReLU_1)\n",
    "\n",
    "# parameters for conv_2\n",
    "W_conv_2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv_2 = bias_variable([64])\n",
    "\n",
    "conv_2 = conv2d(pool_1, W_conv_2) + b_conv_2\n",
    "ReLU_2 = tf.nn.relu(conv_2)\n",
    "pool_2 = max_pool_2x2(ReLU_2)\n",
    "\n",
    "reshape = tf.reshape(pool_2, [-1, 7*7*64])\n",
    "\n",
    "# parameters for fc_1\n",
    "W_fc_1 = weight_variable([7*7*64, 1024])\n",
    "b_fc_1 = bias_variable([1024])\n",
    "fc_1 = tf.matmul(reshape, W_fc_1) + b_fc_1\n",
    "ReLU_3 = tf.nn.relu(fc_1)\n",
    "dropout = tf.nn.dropout(ReLU_3, keep_prob)\n",
    "\n",
    "# parameters for fc_2\n",
    "W_fc_2 = weight_variable([1024, 10])\n",
    "b_fc_2 = bias_variable([10])\n",
    "# output of fc_2\n",
    "y_logits = tf.matmul(dropout, W_fc_2) + b_fc_2\n",
    "\n",
    "# loss\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_input, \n",
    "                                            logits=y_logits))\n",
    "# minimizaer to minimize cross_entropy\n",
    "obj = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# calculate prediction accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_logits, 1), \n",
    "                              tf.argmax(y_input, 1)  )\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, accuracy:0.1051\n",
      "Step:100, accuracy:0.8888\n",
      "Step:200, accuracy:0.9280\n",
      "Step:300, accuracy:0.9417\n",
      "Step:400, accuracy:0.9499\n",
      "Step:500, accuracy:0.9546\n",
      "Step:600, accuracy:0.9584\n",
      "Step:700, accuracy:0.9650\n",
      "Step:800, accuracy:0.9660\n",
      "Step:900, accuracy:0.9694\n",
      "Step:1000, accuracy:0.9694\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1001):\n",
    "        batch_img, batch_label = mnist.train.next_batch(100)\n",
    "        sess.run(obj, feed_dict={x_input: batch_img, \n",
    "                                 y_input: batch_label, \n",
    "                                 keep_prob: 0.5} )\n",
    "        if i % 100 == 0:\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x_input: mnist.test.images, \n",
    "                                                           y_input: mnist.test.labels,\n",
    "                                                           keep_prob: 1})\n",
    "            print(\"Step:{}, accuracy:{:.4f}\".format(i, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
